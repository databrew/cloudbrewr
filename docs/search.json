[{"path":"/articles/cloudbrewr-intro.html","id":"log-in-to-aws-databrew","dir":"Articles","previous_headings":"","what":"Log in to AWS @DataBrew","title":"CloudBrewR S3 Introduction","text":"Let’s take running login, depending session two login types. Interactive Session (RStudio, RSession): Function trigger SSO login DataBrew AWS Portal Bash / EC2 / Containers: Function prompt user export access keys directly ourDataBrew AWS Portal. representative production data pipeline uses IAM Roles","code":"aws_login(     role_name = 'cloudbrewr-aws-role',     profile_name = 'dbrew-cloudbrewr-aws-role')"},{"path":"/articles/cloudbrewr-intro.html","id":"get-an-object-from-s3","dir":"Articles","previous_headings":"","what":"Get an Object from S3","title":"CloudBrewR S3 Introduction","text":"Running object getter give user metadata object, starting item size content_length, version_id, s3-related information. read .csv file locally run: default, data save ~/.cloudbrew_cache, change existing directory use output_dir parameter:","code":"aws_s3_get_object(   bucket = 'databrew.org',    key = 'bohemiatesting-ke/raw-form/reconhh/reconhh.csv') #> $bucket #> databrew.org #>  #> $object_key #> [1] \"bohemiatesting-ke/raw-form/reconhh/reconhh.csv\" #>  #> $last_modified #> [1] \"2022-11-30 00:00:34 GMT\" #>  #> $content_length #> [1] 3089 #>  #> $version_id #> [1] \"gsSHUQeAfyWCpoApw5YKujw98FHT6MxW\" #>  #> $content_type #> [1] \"binary/octet-stream\" #>  #> $etag #> [1] \"ab1be5f1476a9a687165fe655fc2d53d\" #>  #> $file_path #> [1] \"~/.cloudbrewr_cache/reconhh.csv\"  object <- aws_s3_get_object(   bucket = 'databrew.org',    key = 'bohemiatesting-ke/raw-form/reconhh/reconhh.csv') data <- fread(object$file_path) %>%    dplyr::select(SubmissionDate) %>%   head(5) data #>         SubmissionDate #> 1: 2022-07-04 19:53:41 #> 2: 2022-07-04 12:53:50 #> 3: 2022-07-04 12:36:28 #> 4: 2022-07-04 12:22:13 #> 5: 2022-06-29 04:02:33 object <- aws_s3_get_object(   bucket = 'databrew.org',    key = 'bohemiatesting-ke/raw-form/reconhh/reconhh.csv',   output_dir = 'data')"},{"path":"/articles/cloudbrewr-intro.html","id":"storing-an-object-to-s3-bucket-internal-use","dir":"Articles","previous_headings":"","what":"Storing an Object to S3 bucket (@Internal Use)","title":"CloudBrewR S3 Introduction","text":"Say want save output S3, procedure store data:","code":"tempfile <- tempfile(fileext = \".csv\") data %>% fwrite(tempfile) aws_s3_store(   filename = tempfile,    bucket = 'databrew.org',    key = 'bohemiatesting-ke/test.csv',   namespace_bucket = TRUE) #> [CLOUDBREWR_LOGS]: File is uploaded with in bucket:databrew.org; key:bohemiatesting-ke/test.csv"},{"path":"/articles/cloudbrewr-three-dot-functions.html","id":"what-is-dot-dot-dot-function-in-r","dir":"Articles","previous_headings":"","what":"What is Dot Dot Dot function in R?","title":"CloudBrewR Three Dot Ellipsis Parameter","text":"means function designed take number named unnamed arguments. Just like Python *kwargs parameter","code":""},{"path":"/articles/cloudbrewr-three-dot-functions.html","id":"how-do-we-use-it-in-this-library","dir":"Articles","previous_headings":"","what":"How do we use it in this library?","title":"CloudBrewR Three Dot Ellipsis Parameter","text":"Say getting object S3 realize might need extra parameter s3 get object. aws_s3_get_object built ..., can find extra parameter can use Paws official API Docs. , able modify S3 get specific versioning needs :","code":"aws_s3_get_object(   bucket = 'databrew.org',   key = 'kwale/raw-form/reconbhousehold/reconbhousehold.csv',   VersionId = 'NaFvmYr2_E3x57xPBMmyvaK6Ar3fi1k7' # represents the 3 dots ) %>% head(5) #> $bucket #> databrew.org #>  #> $object_key #> [1] \"kwale/raw-form/reconbhousehold/reconbhousehold.csv\" #>  #> $last_modified #> [1] \"2023-01-06 21:02:12 GMT\" #>  #> $content_length #> [1] 4722148 #>  #> $version_id #> [1] \"NaFvmYr2_E3x57xPBMmyvaK6Ar3fi1k7\""},{"path":"/articles/cloudbrewr-utility-functions.html","id":"about","dir":"Articles","previous_headings":"","what":"About","title":"CloudBrewR S3 Utility Functions","text":"section covers utility function used can help daily S3 usage. utility functions CloudBrewR","code":""},{"path":"/articles/cloudbrewr-utility-functions.html","id":"bulk-get-s3-objects","dir":"Articles","previous_headings":"About","what":"Bulk Get S3 Objects","title":"CloudBrewR S3 Utility Functions","text":"function used bulk data retrieval bulk retrieval operation, able store output object go file mapping ~/.cloudbrewr_cache existing directory set custom output_dir","code":"object <- cloudbrewr::aws_s3_bulk_get(     bucket = 'bohemia-datalake',     Prefix = 'bohemia-minicensus' ) #> [CLOUDBREWR_LOGS]: Downloading 8 Objects in S3..  object #> # A tibble: 8 × 4 #>   file_path                                                   etag  key   bucket #>   <fs::path>                                                  <chr> <chr> <chr>  #> 1 ~/.cloudbrewr_cache/bohemia-minicensus                      d41d… bohe… bohem… #> 2 ~/.cloudbrewr_cache/clean_minicensus_main.csv               1aea… bohe… bohem… #> 3 ~/.cloudbrewr_cache/clean_minicensus_people.csv             5c18… bohe… bohem… #> 4 ~/.cloudbrewr_cache/clean_minicensus_repeat_death_info.csv  2faf… bohe… bohem… #> 5 ~/.cloudbrewr_cache/clean_minicensus_repeat_hh_sub.csv      335a… bohe… bohem… #> 6 ….cloudbrewr_cache/clean_minicensus_repeat_mosquito_net.csv 9aed… bohe… bohem… #> 7 ~/.cloudbrewr_cache/clean_minicensus_repeat_water.csv       12fb… bohe… bohem… #> 8 ~/.cloudbrewr_cache/clean_refusals.csv                      3bb2… bohe… bohem…"},{"path":"/articles/cloudbrewr-utility-functions.html","id":"get-s3-data-catalog","dir":"Articles","previous_headings":"About","what":"Get S3 Data Catalog","title":"CloudBrewR S3 Utility Functions","text":"function used get catalog files databrew.org bucket. crawl objects inside S3 folder narrow catalog search:","code":"aws_s3_get_catalog(   bucket = 'databrew.org') %>%    head(5) %>%    dplyr::select(Key, LastModified, Size) #> Warning: Expected 3 pieces. Additional pieces discarded in 192 rows [27, 28, 29, 30, 31, #> 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, ...]. #> Warning: Expected 3 pieces. Missing pieces filled with `NA` in 22 rows [1, 2, 3, 4, 5, #> 6, 7, 8, 9, 10, 11, 12, 21, 22, 23, 24, 219, 220, 245, 246, ...]. #> # A tibble: 5 × 3 #>   Key                                          LastModified             Size #>   <chr>                                        <dttm>                  <dbl> #> 1 bohemia-analyses/                            2023-02-28 15:34:49         0 #> 2 bohemia-analyses/                            2023-02-28 15:34:49         0 #> 3 bohemia-analyses/vaccine_analysis/           2023-02-28 15:35:09         0 #> 4 bohemia-analyses/vaccine_analysis/           2023-02-28 15:35:09         0 #> 5 bohemia-analyses/vaccine_analysis/data.RData 2023-02-28 15:36:01 178452095 aws_s3_get_catalog(   bucket = 'databrew.org',   Prefix = 'kwale/clean-form/') %>%    head(5) %>%    dplyr::select(Key, LastModified, Size) #> # A tibble: 5 × 3 #>   Key                                                 LastModified          Size #>   <chr>                                               <dttm>               <dbl> #> 1 kwale/clean-form/reconaregistration/reconaregistra… 2023-03-14 06:04:25 1.24e5 #> 2 kwale/clean-form/reconaregistration/reconaregistra… 2023-03-14 06:04:25 1.24e5 #> 3 kwale/clean-form/reconaregistrationtraining/recona… 2022-11-30 00:02:07 1.27e4 #> 4 kwale/clean-form/reconaregistrationtraining/recona… 2022-11-30 00:02:07 1.27e4 #> 5 kwale/clean-form/reconbhousehold/reconbhousehold.c… 2023-03-14 06:04:32 9.34e6"},{"path":"/articles/cloudbrewr-utility-functions.html","id":"get-s3-object-versioning-history","dir":"Articles","previous_headings":"About","what":"Get S3 Object Versioning History","title":"CloudBrewR S3 Utility Functions","text":"function used get object versioning history (notated VersionId) metadata","code":"aws_s3_get_object_version_history(   bucket = 'databrew.org',    key = 'kwale/raw-form/reconaregistration/reconaregistration.csv') %>%    head(5) %>%    dplyr::select(Key, Size, LastModified, VersionId) #> # A tibble: 5 × 4 #>   Key                                           Size LastModified        Versi…¹ #>   <chr>                                        <dbl> <dttm>              <chr>   #> 1 kwale/raw-form/reconaregistration/reconare… 135680 2023-03-14 06:02:37 fTKmHJ… #> 2 kwale/raw-form/reconaregistration/reconare… 135680 2023-03-14 05:02:16 PBNDMz… #> 3 kwale/raw-form/reconaregistration/reconare… 135680 2023-03-13 21:02:16 XgBKu2… #> 4 kwale/raw-form/reconaregistration/reconare… 135680 2023-03-13 14:02:32 _S2EwH… #> 5 kwale/raw-form/reconaregistration/reconare… 135680 2023-03-13 13:02:17 ss3V4D… #> # … with abbreviated variable name ¹​VersionId"},{"path":"/articles/cloudbrewr-utility-functions.html","id":"get-table-directly-from-s3","dir":"Articles","previous_headings":"About","what":"Get Table Directly from S3","title":"CloudBrewR S3 Utility Functions","text":"Wrapper function get table-like data directly S3","code":"aws_s3_get_table(   bucket = 'databrew.org',    key = 'bohemiatesting-ke/raw-form/reconregistration/reconregistration.csv') %>%    head(5) %>%    dplyr::select(SubmissionDate) #> # A tibble: 5 × 1 #>   SubmissionDate      #>   <dttm>              #> 1 2022-09-20 10:54:24 #> 2 2022-07-04 19:50:22 #> 3 2022-07-04 13:03:52 #> 4 2022-07-04 12:20:32 #> 5 2022-06-30 09:55:45"},{"path":"/articles/cloudbrewr-utility-functions.html","id":"get-files-partitioned-in-folder","dir":"Articles","previous_headings":"About","what":"Get Files Partitioned in Folder","title":"CloudBrewR S3 Utility Functions","text":"working Data Lakes S3, one commons strategy save .csv files folders/partitions. One use-case storing Anomalies per Day. structured S3 partitions, able take snapshots many anomalies identified Recon, format follows: databrew.org/kwale/anomalies/anomalies-identification-history/run_date={YYYY}-{MM}-{DD} can query data rowwwise format running, indexed run_date running can also repurpose fetch specific date-range -extract partitioned files.","code":"aws_s3_get_table_ts(   bucket = 'databrew.org',    key = 'kwale/anomalies/anomalies-identification-history/') %>%    head(5) %>%    dplyr::select(run_date,type) #> # A tibble: 5 × 2 #>   run_date   type                #>   <IDate>    <chr>               #> 1 2022-12-07 recona_duplicate_id #> 2 2022-12-07 recona_duplicate_id #> 3 2022-12-07 reconb_duplicate_id #> 4 2022-12-07 reconb_duplicate_id #> 5 2022-12-07 reconb_duplicate_id aws_s3_get_table_ts(   bucket = 'databrew.org',    key = 'kwale/anomalies/anomalies-identification-history/',   date_range = c('2023-01-01', '2023-01-05')) %>%    head(5) %>%    dplyr::select(run_date,type) #> # A tibble: 5 × 2 #>   run_date   type                              #>   <IDate>    <chr>                             #> 1 2023-01-01 recona_mismatch_cha_supervise_chv #> 2 2023-01-01 recona_mismatch_cha_supervise_chv #> 3 2023-01-02 recona_mismatch_cha_supervise_chv #> 4 2023-01-02 recona_mismatch_cha_supervise_chv #> 5 2023-01-03 recona_mismatch_cha_supervise_chv"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Aryton Tediarjo. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Tediarjo (2023). cloudbrewr: DataBrew Internal Tooling AWS Data Load / Storage. R package version 0.0.0.9000.","code":"@Manual{,   title = {cloudbrewr: DataBrew Internal Tooling for AWS Data Load / Storage},   author = {Aryton Tediarjo},   year = {2023},   note = {R package version 0.0.0.9000}, }"},{"path":"/index.html","id":"cloudbrewr","dir":"","previous_headings":"","what":"DataBrew Internal Tooling for AWS Data Load / Storage","title":"DataBrew Internal Tooling for AWS Data Load / Storage","text":"@Author: atediarjo@gmail.com Internal Tooling DataBrew Cloud ETL. purpose R package enable user able retrieve store research data scale DataBrew AWS Environment.","code":""},{"path":"/index.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"DataBrew Internal Tooling for AWS Data Load / Storage","text":"enable cloudbrewr run machine, make sure AWS CLI installed. Installation Link SSO Access - please reach DataBrew team (atediarjo@gmail.com, joebrew@gmail.com)","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"DataBrew Internal Tooling for AWS Data Load / Storage","text":"Installation can done Github installation:","code":"devtools::install_github('databrew/cloudbrewr')"},{"path":"/index.html","id":"log-in-to-aws","dir":"","previous_headings":"","what":"Log in to AWS","title":"DataBrew Internal Tooling for AWS Data Load / Storage","text":"Pass role name arbitrary based user. example, bohemia-box-team-s3-role box access bohemia-ento-team-s3-role ento folder access run code snippet, redirected AWS SSO Portal default browser Input username password created email invitation Click Allow following window prompt Voila! R session now connected AWS command create profile ~/.aws/config, AWS use reference future authentication","code":"library(cloudbrewr) cloudbrewr::aws_login(   role_name = 'SSO_ROLE_NAME_FROM_WEB_PORTAL',   profile_name = 'AWS_PROFILE_NAME_OF_CHOICE' )"},{"path":"/index.html","id":"fetching-your-first-object","dir":"","previous_headings":"","what":"Fetching your first object","title":"DataBrew Internal Tooling for AWS Data Load / Storage","text":"example can get S3 object read .csv file","code":"object <- cloudbrewr::aws_s3_get_object(     bucket = 'bohemia-datalake',     key = 'bohemia-minicensus/clean_minicensus_main.csv', )  # read based on object metadata read.csv(object$file_path)"},{"path":"/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"DataBrew Internal Tooling for AWS Data Load / Storage","text":"read features, check team Vignettes Github Pages","code":""},{"path":"/index.html","id":"contribute","dir":"","previous_headings":"","what":"Contribute","title":"DataBrew Internal Tooling for AWS Data Load / Storage","text":"-Dev atediarjo@gmail.com","code":""},{"path":"/reference/aws_configure.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to create SSO configuration — aws_configure","title":"Function to create SSO configuration — aws_configure","text":"Create ~/.aws/config requirements SSO DataBrew AWS Accounts","code":""},{"path":"/reference/aws_configure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to create SSO configuration — aws_configure","text":"","code":"aws_configure(env)"},{"path":"/reference/aws_configure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to create SSO configuration — aws_configure","text":"env environment variables","code":""},{"path":"/reference/aws_login.html","id":null,"dir":"Reference","previous_headings":"","what":"Utility Function to login to DataBrew AWS. — aws_login","title":"Utility Function to login to DataBrew AWS. — aws_login","text":"user interactive session, generate required credentials run SSO. user running Terminal / bash / workflow / VM prompt export temporary credentials","code":""},{"path":"/reference/aws_login.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Utility Function to login to DataBrew AWS. — aws_login","text":"","code":"aws_login(pipeline_stage = \"production\")"},{"path":"/reference/aws_login.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Utility Function to login to DataBrew AWS. — aws_login","text":"pipeline_stage (optional) choose production/develop stage","code":""},{"path":"/reference/aws_login.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Utility Function to login to DataBrew AWS. — aws_login","text":"metadata AWS STS authentication (Account, Role)","code":""},{"path":"/reference/aws_namespace.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to namespace bucket — aws_namespace","title":"Function to namespace bucket — aws_namespace","text":"function used namespace bucket based pipelines stages","code":""},{"path":"/reference/aws_namespace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to namespace bucket — aws_namespace","text":"","code":"aws_namespace(bucket)"},{"path":"/reference/aws_namespace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to namespace bucket — aws_namespace","text":"bucket databrew s3 bucket","code":""},{"path":"/reference/aws_namespace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to namespace bucket — aws_namespace","text":"namspaced buckeet","code":""},{"path":"/reference/aws_s3_bulk_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Bulk Get Object in DataBrew S3 — aws_s3_bulk_get","title":"Bulk Get Object in DataBrew S3 — aws_s3_bulk_get","text":"Bulk Get Object DataBrew S3","code":""},{"path":"/reference/aws_s3_bulk_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bulk Get Object in DataBrew S3 — aws_s3_bulk_get","text":"","code":"aws_s3_bulk_get(   bucket,   namespace_bucket = TRUE,   build_metadata = TRUE,   output_dir = NULL,   ... )"},{"path":"/reference/aws_s3_bulk_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bulk Get Object in DataBrew S3 — aws_s3_bulk_get","text":"bucket s3 bucket namespace_bucket boolean create namespace bucket, set FALSE override bucket namespace build_metadata boolean create metadata bulk object retrieval S3 output_dir output directory aws_s3_get parameter ... additional parameter S3 get object","code":""},{"path":"/reference/aws_s3_bulk_get.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bulk Get Object in DataBrew S3 — aws_s3_bulk_get","text":"list object s3 metadata file location","code":""},{"path":"/reference/aws_s3_create_bucket.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to create S3 bucket — aws_s3_create_bucket","title":"Function to create S3 bucket — aws_s3_create_bucket","text":"create S3 bucket via API","code":""},{"path":"/reference/aws_s3_create_bucket.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to create S3 bucket — aws_s3_create_bucket","text":"","code":"aws_s3_create_bucket(bucket, namespace_bucket = TRUE, ...)"},{"path":"/reference/aws_s3_create_bucket.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to create S3 bucket — aws_s3_create_bucket","text":"bucket s3 bucket namespace_bucket namespace bucket prod/dev ... additional parameter passed s3 create_bucket endpoint","code":""},{"path":"/reference/aws_s3_get_catalog.html","id":null,"dir":"Reference","previous_headings":"","what":"Get DataBrew S3 object catalog — aws_s3_get_catalog","title":"Get DataBrew S3 object catalog — aws_s3_get_catalog","text":"Get catalog files used S3","code":""},{"path":"/reference/aws_s3_get_catalog.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get DataBrew S3 object catalog — aws_s3_get_catalog","text":"","code":"aws_s3_get_catalog(bucket, namespace_bucket = TRUE, ...)"},{"path":"/reference/aws_s3_get_catalog.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get DataBrew S3 object catalog — aws_s3_get_catalog","text":"bucket s3 bucket namespace_bucket boolean create namespace bucket, set FALSE override bucket namespace ... additional parameter S3 get object","code":""},{"path":"/reference/aws_s3_get_catalog.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get DataBrew S3 object catalog — aws_s3_get_catalog","text":"tibble S3 file catalog","code":""},{"path":"/reference/aws_s3_get_header.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Metadata Headers — aws_s3_get_header","title":"Get Metadata Headers — aws_s3_get_header","text":"Get Metadata Headers","code":""},{"path":"/reference/aws_s3_get_header.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Metadata Headers — aws_s3_get_header","text":"","code":"aws_s3_get_header(bucket, key, namespace_bucket = TRUE)"},{"path":"/reference/aws_s3_get_header.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Metadata Headers — aws_s3_get_header","text":"bucket s3 bucket key s3 object key namespace_bucket boolean create namespace bucket, set FALSE override bucket namespace ... additional parameter S3 get object","code":""},{"path":"/reference/aws_s3_get_header.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Metadata Headers — aws_s3_get_header","text":"list object s3 metadata file location","code":""},{"path":"/reference/aws_s3_get_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Object in DataBrew S3 — aws_s3_get_object","title":"Get Object in DataBrew S3 — aws_s3_get_object","text":"Get Object DataBrew S3","code":""},{"path":"/reference/aws_s3_get_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Object in DataBrew S3 — aws_s3_get_object","text":"","code":"aws_s3_get_object(bucket, key, output_dir = NULL, namespace_bucket = TRUE, ...)"},{"path":"/reference/aws_s3_get_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Object in DataBrew S3 — aws_s3_get_object","text":"bucket s3 bucket key s3 object key output_dir output directory, create tempfile set null namespace_bucket boolean create namespace bucket, set FALSE override bucket namespace ... additional parameter S3 get object","code":""},{"path":"/reference/aws_s3_get_object.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Object in DataBrew S3 — aws_s3_get_object","text":"list object s3 metadata file location","code":""},{"path":"/reference/aws_s3_get_object_version_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Get object version history in s3 — aws_s3_get_object_version_history","title":"Get object version history in s3 — aws_s3_get_object_version_history","text":"Get object version change history","code":""},{"path":"/reference/aws_s3_get_object_version_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get object version history in s3 — aws_s3_get_object_version_history","text":"","code":"aws_s3_get_object_version_history(bucket, key, namespace_bucket = TRUE, ...)"},{"path":"/reference/aws_s3_get_object_version_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get object version history in s3 — aws_s3_get_object_version_history","text":"bucket s3 bucket key s3 object key namespace_bucket boolean create namespace bucket, set FALSE override bucket namespace ... additional parameter S3 get object","code":""},{"path":"/reference/aws_s3_get_object_version_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get object version history in s3 — aws_s3_get_object_version_history","text":"tibble S3 object version history","code":""},{"path":"/reference/aws_s3_get_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to retrieve s3 object directly into table — aws_s3_get_table","title":"Function to retrieve s3 object directly into table — aws_s3_get_table","text":"wrapper function transform s3 tibble","code":""},{"path":"/reference/aws_s3_get_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to retrieve s3 object directly into table — aws_s3_get_table","text":"","code":"aws_s3_get_table(bucket, key, namespace_bucket = TRUE, ...)"},{"path":"/reference/aws_s3_get_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to retrieve s3 object directly into table — aws_s3_get_table","text":"bucket s3 bucket key s3 object key namespace_bucket boolean create namespace bucket, set FALSE override bucket namespace ... additional parameter S3 get object","code":""},{"path":"/reference/aws_s3_get_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to retrieve s3 object directly into table — aws_s3_get_table","text":"tibble data-frame s3 object","code":""},{"path":"/reference/aws_s3_get_table_ts.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to retrieve S3 folder-partitioned table into time-series data — aws_s3_get_table_ts","title":"Function to retrieve S3 folder-partitioned table into time-series data — aws_s3_get_table_ts","text":"Wrapper bind folder-partitioned table time-series, indexed run_date","code":""},{"path":"/reference/aws_s3_get_table_ts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to retrieve S3 folder-partitioned table into time-series data — aws_s3_get_table_ts","text":"","code":"aws_s3_get_table_ts(   bucket,   key,   namespace_bucket = TRUE,   date_range = NULL,   ... )"},{"path":"/reference/aws_s3_get_table_ts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to retrieve S3 folder-partitioned table into time-series data — aws_s3_get_table_ts","text":"bucket s3 bucket key object key namespace_bucket boolean bucket namespacing, set FALSE override namespace date_range date range min/max date partition ... argument s3 api list_objects_v2","code":""},{"path":"/reference/aws_s3_get_table_ts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to retrieve S3 folder-partitioned table into time-series data — aws_s3_get_table_ts","text":"tibble dataframe run-date index","code":""},{"path":"/reference/aws_s3_store.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to store object to S3 — aws_s3_store","title":"Function to store object to S3 — aws_s3_store","text":"Store object S3 (limited <=5GB per object)","code":""},{"path":"/reference/aws_s3_store.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to store object to S3 — aws_s3_store","text":"","code":"aws_s3_store(filename, bucket, key, namespace_bucket = TRUE, ...)"},{"path":"/reference/aws_s3_store.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to store object to S3 — aws_s3_store","text":"filename filename bucket s3 bucket key s3 object key namespace_bucket boolean whether namespace bucket based prod/dev environment ... additional parameter passed s3 put_object","code":""},{"path":"/reference/aws_s3_sync.html","id":null,"dir":"Reference","previous_headings":"","what":"bucket, key, — aws_s3_sync","title":"bucket, key, — aws_s3_sync","text":"bucket, key,","code":""},{"path":"/reference/aws_s3_sync.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"bucket, key, — aws_s3_sync","text":"","code":"aws_s3_sync(manifest)"},{"path":"/reference/aws_sso_authenticate.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to initiate SSO authentication — aws_sso_authenticate","title":"Function to initiate SSO authentication — aws_sso_authenticate","text":"Instantiate SSO connection using internet browser","code":""},{"path":"/reference/aws_sso_authenticate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to initiate SSO authentication — aws_sso_authenticate","text":"","code":"aws_sso_authenticate(profile_name)"},{"path":"/reference/aws_sso_authenticate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to initiate SSO authentication — aws_sso_authenticate","text":"profile_name profile name set prod/dev","code":""},{"path":"/reference/call_cloudbrewr_stage_env_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to get prod/dev stage environment variables, — call_cloudbrewr_stage_env_variables","title":"Function to get prod/dev stage environment variables, — call_cloudbrewr_stage_env_variables","text":"Retrieve environment variables used S3 bucket namespace profile name environment variables.","code":""},{"path":"/reference/call_cloudbrewr_stage_env_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to get prod/dev stage environment variables, — call_cloudbrewr_stage_env_variables","text":"","code":"call_cloudbrewr_stage_env_variables(pipeline_stage)"},{"path":"/reference/call_cloudbrewr_stage_env_variables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to get prod/dev stage environment variables, — call_cloudbrewr_stage_env_variables","text":"pipeline_stage choose production/develop stage","code":""},{"path":"/reference/check_aws_access.html","id":null,"dir":"Reference","previous_headings":"","what":"Check aws access via STS — check_aws_access","title":"Check aws access via STS — check_aws_access","text":"Check aws access via STS","code":""},{"path":"/reference/check_aws_access.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check aws access via STS — check_aws_access","text":"","code":"check_aws_access()"},{"path":"/reference/check_aws_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check aws access via STS — check_aws_access","text":"metadata STS caller identity","code":""},{"path":"/reference/check_aws_environment_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to check AWS environment variables — check_aws_environment_variables","title":"Function to check AWS environment variables — check_aws_environment_variables","text":"send user message completion AWS Environment Variables","code":""},{"path":"/reference/check_aws_environment_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to check AWS environment variables — check_aws_environment_variables","text":"","code":"check_aws_environment_variables()"}]
